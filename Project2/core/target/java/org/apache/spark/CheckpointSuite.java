package org.apache.spark;
public  class CheckpointSuite extends org.scalatest.FunSuite implements org.apache.spark.LocalSparkContext, org.apache.spark.Logging {
  static public <K extends java.lang.Object, V extends java.lang.Object> org.apache.spark.rdd.RDD<scala.Tuple2<K, scala.collection.Iterable<V>[]>> cogroup (org.apache.spark.rdd.RDD<scala.Tuple2<K, V>> first, org.apache.spark.rdd.RDD<scala.Tuple2<K, V>> second, org.apache.spark.Partitioner part) { throw new RuntimeException(); }
  public   CheckpointSuite () { throw new RuntimeException(); }
  public  java.io.File checkpointDir () { throw new RuntimeException(); }
  public  org.apache.spark.HashPartitioner partitioner () { throw new RuntimeException(); }
  public  void beforeEach () { throw new RuntimeException(); }
  public  void afterEach () { throw new RuntimeException(); }
  public <T extends java.lang.Object> Object defaultCollectFunc (org.apache.spark.rdd.RDD<T> rdd) { throw new RuntimeException(); }
  /**
   * Test checkpointing of the RDD generated by the given operation. It tests whether the
   * serialized size of the RDD is reduce after checkpointing or not. This function should be called
   * on all RDDs that have a parent RDD (i.e., do not call on ParallelCollection, BlockRDD, etc.).
   * <p>
   * @param op an operation to run on the RDD
   * @param collectFunc a function for collecting the values in the RDD, in case there are
   *   non-comparable types like arrays that we want to convert to something that supports ==
   */
  public <U extends java.lang.Object> void testRDD (scala.Function1<org.apache.spark.rdd.RDD<java.lang.Object>, org.apache.spark.rdd.RDD<U>> op, scala.Function1<org.apache.spark.rdd.RDD<U>, java.lang.Object> collectFunc, scala.reflect.ClassTag<U> evidence$1) { throw new RuntimeException(); }
  /**
   * Test whether checkpointing of the parent of the generated RDD also
   * truncates the lineage or not. Some RDDs like CoGroupedRDD hold on to its parent
   * RDDs partitions. So even if the parent RDD is checkpointed and its partitions changed,
   * the generated RDD will remember the partitions and therefore potentially the whole lineage.
   * This function should be called only those RDD whose partitions refer to parent RDD's
   * partitions (i.e., do not call it on simple RDD like MappedRDD).
   * <p>
   * @param op an operation to run on the RDD
   * @param collectFunc a function for collecting the values in the RDD, in case there are
   *   non-comparable types like arrays that we want to convert to something that supports ==
   */
  public <U extends java.lang.Object> void testRDDPartitions (scala.Function1<org.apache.spark.rdd.RDD<java.lang.Object>, org.apache.spark.rdd.RDD<U>> op, scala.Function1<org.apache.spark.rdd.RDD<U>, java.lang.Object> collectFunc, scala.reflect.ClassTag<U> evidence$2) { throw new RuntimeException(); }
  /**
   * Generate an RDD such that both the RDD and its partitions have large size.
   */
  public  org.apache.spark.rdd.RDD<java.lang.Object> generateFatRDD () { throw new RuntimeException(); }
  /**
   * Generate an pair RDD (with partitioner) such that both the RDD and its partitions
   * have large size.
   */
  public  org.apache.spark.rdd.RDD<scala.Tuple2<java.lang.Object, java.lang.Object>> generateFatPairRDD () { throw new RuntimeException(); }
  /**
   * Get serialized sizes of the RDD and its partitions, in order to test whether the size shrinks
   * upon checkpointing. Ignores the checkpointData field, which may grow when we checkpoint.
   */
  public  scala.Tuple2<java.lang.Object, java.lang.Object> getSerializedSizes (org.apache.spark.rdd.RDD<?> rdd) { throw new RuntimeException(); }
  /**
   * Serialize and deserialize an object. This is useful to verify the objects
   * contents after deserialization (e.g., the contents of an RDD split after
   * it is sent to a slave along with a task)
   */
  public <T extends java.lang.Object> T serializeDeserialize (T obj) { throw new RuntimeException(); }
  /**
   * Recursively force the initialization of the all members of an RDD and it parents.
   */
  public  void initializeRdd (org.apache.spark.rdd.RDD<?> rdd) { throw new RuntimeException(); }
}
